{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "gym_test_pong.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOoQtTYCaToG7mSeD5Hq9kx",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/siddhant012/Open-AI-Gym-colab/blob/main/gym_test_pong.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "__WMJDlMBabm",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 404
        },
        "outputId": "ea7c8670-455c-411b-e87e-f4fc3fe15d6e"
      },
      "source": [
        "!pip install pyvirtualdisplay\n",
        "!apt install xvfb -y\n",
        "import gym\n",
        "from pyvirtualdisplay import Display\n",
        "from IPython.display import display\n",
        "from IPython.display import clear_output\n",
        "import PIL\n",
        "import time\n",
        "import numpy as np\n",
        "import random\n",
        "import math\n",
        "import sys\n",
        "import re\n",
        "from tensorflow import keras\n",
        "from cv2 import resize as cv2_resize\n",
        "#Display(visible=0, size=(1400,900)).start()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting pyvirtualdisplay\n",
            "  Downloading https://files.pythonhosted.org/packages/d0/8a/643043cc70791367bee2d19eb20e00ed1a246ac48e5dbe57bbbcc8be40a9/PyVirtualDisplay-1.3.2-py2.py3-none-any.whl\n",
            "Collecting EasyProcess\n",
            "  Downloading https://files.pythonhosted.org/packages/48/3c/75573613641c90c6d094059ac28adb748560d99bd27ee6f80cce398f404e/EasyProcess-0.3-py2.py3-none-any.whl\n",
            "Installing collected packages: EasyProcess, pyvirtualdisplay\n",
            "Successfully installed EasyProcess-0.3 pyvirtualdisplay-1.3.2\n",
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "The following NEW packages will be installed:\n",
            "  xvfb\n",
            "0 upgraded, 1 newly installed, 0 to remove and 21 not upgraded.\n",
            "Need to get 783 kB of archives.\n",
            "After this operation, 2,266 kB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 xvfb amd64 2:1.19.6-1ubuntu4.7 [783 kB]\n",
            "Fetched 783 kB in 1s (695 kB/s)\n",
            "Selecting previously unselected package xvfb.\n",
            "(Reading database ... 144611 files and directories currently installed.)\n",
            "Preparing to unpack .../xvfb_2%3a1.19.6-1ubuntu4.7_amd64.deb ...\n",
            "Unpacking xvfb (2:1.19.6-1ubuntu4.7) ...\n",
            "Setting up xvfb (2:1.19.6-1ubuntu4.7) ...\n",
            "Processing triggers for man-db (2.8.3-2ubuntu0.1) ...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PoqZfIsGmb79"
      },
      "source": [
        "class PolicyNet:\n",
        "\n",
        "    def __init__(self,load_name=\"\",save_name=\"\"):\n",
        "\n",
        "        self.load_name=load_name\n",
        "        self.save_name=save_name\n",
        "        self.learning_rate=0.01\n",
        "        self.loss=\"mse\"\n",
        "        self.policy=None\n",
        "    \n",
        "    def build_model(self,input_shape,output_shape,learning_rate=0.01,loss=\"mse\"):\n",
        "        self.learning_rate=learning_rate\n",
        "        self.loss=loss\n",
        "\n",
        "        if(len(self.load_name)>0):\n",
        "            self.model=keras.models.load_model(self.load_name)\n",
        "        else:\n",
        "            '''self.model=keras.Sequential()\n",
        "            self.model.add(keras.layers.Dense(50,input_shape=input_shape,activation=\"relu\"))\n",
        "            self.model.add(keras.layers.Dense(50,activation=\"relu\"))\n",
        "            self.model.add(keras.layers.Dense(units=output_shape,activation=\"linear\"))'''\n",
        "\n",
        "            self.model=keras.Sequential()\n",
        "            self.model.add(keras.layers.Conv2D(32, kernel_size=(3,3), input_shape=input_shape,activation=\"relu\",padding=\"valid\"))\n",
        "            self.model.add(keras.layers.MaxPool2D(pool_size=(2,2)))\n",
        "            #self.model.add(keras.layers.Conv2D(32, kernel_size=(3,3),activation=\"relu\",padding=\"valid\"))\n",
        "            #self.model.add(keras.layers.MaxPool2D(pool_size=(2,2)))\n",
        "            self.model.add(keras.layers.Flatten())\n",
        "            self.model.add(keras.layers.Dense(128,activation=\"relu\"))\n",
        "            self.model.add(keras.layers.Dense(64,activation=\"relu\"))\n",
        "            self.model.add(keras.layers.Dense(units=output_shape,activation=\"linear\"))\n",
        "\n",
        "            self.model.compile(optimizer=keras.optimizers.Adam(lr=self.learning_rate),loss=self.loss)\n",
        "\n",
        "        return self.model\n",
        "\n",
        "    def preprocess_inputs(self,inputs):\n",
        "        display(PIL.Image.fromarray(inputs[0]))\n",
        "\n",
        "        inputs=inputs.astype(np.float32)\n",
        "        inputs=np.expand_dims(inputs.astype(np.float32).mean(axis=3),axis=3)#/255.0\n",
        "        inputs=inputs[:,30:,:,:]\n",
        "        inputs=np.expand_dims( np.array([cv2_resize(inputs[i],(140,140)) for i in range(len(inputs))]).astype(np.float32),axis=3 )\n",
        "        return inputs\n",
        "\n",
        "    def get_model(self):\n",
        "        return self.model\n",
        "    \n",
        "    def get_model_weights(self):\n",
        "        return self.model.get_weights()\n",
        "\n",
        "    def get_hyperparameters(self):\n",
        "        return dict([('learning_rate',self.learning_rate),('loss',self.loss)])\n",
        "\n",
        "    def save_model(self):\n",
        "        if(len(self.save_name)>0) : self.model.save(self.save_name)\n",
        "    \n",
        "    def train_model(self,inputs,outputs):\n",
        "        inputs=self.preprocess_inputs(inputs)\n",
        "        self.model.fit(inputs.astype(np.float32),outputs.astype(np.float32),epochs=1,batch_size=1,verbose=0,shuffle=False)\n",
        "    \n",
        "    def get_Qvals(self,inputs):\n",
        "        inputs=self.preprocess_inputs(inputs)\n",
        "        Qvals=self.model.predict(inputs.astype(np.float32))\n",
        "        return Qvals\n",
        "    \n",
        "    def get_policy_from_Qvals(self,Qvals):\n",
        "        self.policy=np.exp(Qvals) / np.sum(np.exp(Qvals), axis=0)\n",
        "        return self.policy\n",
        "\n",
        "class TargetNet:\n",
        "\n",
        "    def __init__(self,policy_net,target_update=10):\n",
        "        self.policy_net=policy_net\n",
        "        self.target_update=target_update\n",
        "    \n",
        "    def build_model(self):\n",
        "        self.model=keras.models.clone_model(self.policy_net.model)\n",
        "        hyperparameters=self.policy_net.get_hyperparameters()\n",
        "        learning_rate=hyperparameters['learning_rate']\n",
        "        loss=hyperparameters['loss']\n",
        "\n",
        "        self.model.compile(optimizer=keras.optimizers.Adam(lr=learning_rate),loss=loss)\n",
        "        return self.model\n",
        "\n",
        "    def set_model_weights(self):\n",
        "        self.model.set_weights(self.policy_net.get_model_weights())\n",
        "\n",
        "    def get_target_Qvals(self,inputs,step):\n",
        "        if(step%self.target_update==0) : self.set_model_weights()\n",
        "        inputs=self.policy_net.preprocess_inputs(inputs)\n",
        "        return self.model.predict(inputs.astype(np.float32))\n",
        "\n",
        "\n",
        "class ReplayMemory:\n",
        "    def __init__(self,capacity,batch_size):\n",
        "        self.capacity=capacity\n",
        "        self.batch_size=batch_size\n",
        "        self.memory=[]\n",
        "        self.push_count=0\n",
        "\n",
        "    def push(self,experience):\n",
        "        if(len(self.memory)<self.capacity):\n",
        "            self.memory.append(experience)\n",
        "        else:\n",
        "            self.memory[self.push_count%self.capacity]=experience\n",
        "        self.push_count+=1\n",
        "    \n",
        "    def sample(self):\n",
        "        return random.sample(self.memory,self.batch_size)\n",
        "    \n",
        "    def can_provide_sample(self):\n",
        "        return len(self.memory)>=self.batch_size\n",
        "\n",
        "\n",
        "\n",
        "class ExploreExploitStrat:\n",
        "    def __init__(self,strategy):\n",
        "        self.strategy=strategy\n",
        "\n",
        "    def get_exploration_rate(self,current_step):\n",
        "        return self.strategy.get_exploration_rate(current_step)\n",
        "\n",
        "class EpsilonGreedyStrategy:\n",
        "    def __init__(self,start,end,decay_rate):\n",
        "        self.start=start\n",
        "        self.end=end\n",
        "        self.decay_rate=decay_rate\n",
        "\n",
        "    def get_exploration_rate(self,current_step):\n",
        "        return self.end + (self.start-self.end)*math.exp(-1*current_step*self.decay_rate)\n",
        "\n",
        "\n",
        "class ReturnStrat:\n",
        "    def __init__(self,strategy):\n",
        "        self.strategy=strategy\n",
        "\n",
        "    def get_return(self,rewards,next_Qvals,dones,current_step):\n",
        "        return self.strategy.get_return(rewards,next_Qvals,dones,current_step)\n",
        "\n",
        "class DiscountedReturn:\n",
        "    def __init__(self,Y=0.5):\n",
        "        self.Y=Y\n",
        "\n",
        "    def get_return(self,rewards,next_Qvals,dones,current_step):\n",
        "        return rewards+(1-dones)*self.Y*next_Qvals\n",
        "\n",
        "\n",
        "class EnvManager:\n",
        "    \n",
        "    def __init__(self,env_name):\n",
        "        self.env_name=env_name\n",
        "\n",
        "        self.observation=None\n",
        "        self.done=None\n",
        "        self.env_obj=None\n",
        "        self.display_obj=None\n",
        "        \n",
        "    def reset(self):\n",
        "        self.observation=self.env_obj.reset()\n",
        "        self.done=False\n",
        "    \n",
        "    def load(self):\n",
        "        self.display_obj=Display(visible=0, size=(14,9))\n",
        "        self.display_obj.start()\n",
        "        self.env_obj=gym.make(self.env_name)\n",
        "        return self.env_obj\n",
        "        \n",
        "    def close(self):\n",
        "        #self.display_obj.stop()\n",
        "        self.env_obj.close()\n",
        "\n",
        "    def render(self):\n",
        "        frame=PIL.Image.fromarray(self.env_obj.render(mode='rgb_array'))\n",
        "        clear_output(wait=True)\n",
        "        display(frame)\n",
        "        #time.sleep(0.1)\n",
        "\n",
        "    def get_actions_num(self):\n",
        "        return self.env_obj.action_space.n-4\n",
        "    \n",
        "    def get_observation_shape(self):\n",
        "        return np.array(self.env_obj.state).shape\n",
        "\n",
        "    def get_current_observation(self):\n",
        "        return self.observation\n",
        "    \n",
        "    def change(self,action):\n",
        "        action+=2\n",
        "        if(self.done) : self.reset()\n",
        "\n",
        "        self.observation,reward,self.done,info=self.env_obj.step(action)\n",
        "\n",
        "        self.reward=self.modify_reward(reward,self.observation,self.done)\n",
        "        self.observation=self.modify_observation(self.observation)\n",
        "\n",
        "        return self.observation,self.reward,self.done\n",
        "\n",
        "    def modify_observation(self,observation):\n",
        "        return observation\n",
        "    \n",
        "    def modify_reward(self,reward,observation,done):\n",
        "        #if(done) : reward=-10\n",
        "        #reward = observation[0] + 0.5\n",
        "        #if(observation[0]>=0.5) : reward+=10\n",
        "        return reward\n",
        "\n",
        "\n",
        "\n",
        "class Agent:\n",
        "\n",
        "    def __init__(self,env_manager,replay_mem,explore_exploit_strat,return_strat,policy_net,target_net):\n",
        "        self.env_manager=env_manager\n",
        "        self.replay_mem=replay_mem\n",
        "        self.explore_exploit_strat=explore_exploit_strat\n",
        "        self.return_strat=return_strat\n",
        "        self.policy_net=policy_net\n",
        "        self.target_net=target_net\n",
        "\n",
        "        self.action_approach=\"deterministic\"\n",
        "\n",
        "    def choose_action(self,policy):\n",
        "        if(self.action_approach==\"deterministic\"):\n",
        "            action=np.argmax(policy,axis=0)\n",
        "        elif(self.action_approach==\"stochastic\"):\n",
        "            action=np.random.choice(self.env_manager.get_actions_num(),p=policy)\n",
        "        return action\n",
        "\n",
        "\n",
        "    def take_action(self,current_step):\n",
        "\n",
        "        observation=self.env_manager.get_current_observation()\n",
        "        exploration_rate=self.explore_exploit_strat.get_exploration_rate(current_step)\n",
        "        if(random.random()<exploration_rate) :  \n",
        "            action=random.randrange(self.env_manager.get_actions_num())\n",
        "\n",
        "        else : \n",
        "            curr_Qvals=self.policy_net.get_Qvals(np.expand_dims(observation,axis=0))\n",
        "            curr_policy=self.policy_net.get_policy_from_Qvals(np.squeeze(curr_Qvals))\n",
        "            action=self.choose_action(curr_policy)\n",
        "\n",
        "        next_observation,reward,done=self.env_manager.change(action)\n",
        "        experience=(observation,action,next_observation,reward,done)\n",
        "\n",
        "        return experience\n",
        "\n",
        "    def learn(self,experience,current_step):\n",
        "        self.replay_mem.push(experience)\n",
        "\n",
        "        if(self.replay_mem.can_provide_sample()):\n",
        "            samples=self.replay_mem.sample()\n",
        "            observations,actions,next_observations,rewards,dones = np.array(samples).T\n",
        "                    \n",
        "            observations=np.stack(observations,axis=0)\n",
        "            actions=np.array(actions,dtype=np.int32)\n",
        "            next_observations=np.stack(next_observations,axis=0)\n",
        "            rewards=np.array(rewards,dtype=np.float32)\n",
        "            dones=np.array(dones,dtype=np.int32)\n",
        "\n",
        "            curr_Qvals=self.policy_net.get_Qvals(observations)\n",
        "            next_Qvals=self.target_net.get_target_Qvals(next_observations,current_step)\n",
        "            returns=self.return_strat.get_return(rewards, (np.take_along_axis(next_Qvals,actions[:,None],axis=1)).ravel() ,dones,current_step)\n",
        "            target_Qvals=np.copy(curr_Qvals)\n",
        "            np.put_along_axis(target_Qvals , actions[:,None] , returns[:,None] , axis=1)\n",
        "            \n",
        "            self.policy_net.train_model(observations,target_Qvals)\n",
        "\n",
        "\n",
        "\n",
        "class RL:\n",
        "\n",
        "    def __init__(self,agent,env_manager,episodes_num=100,steps_per_episode=200):\n",
        "        self.agent=agent\n",
        "        self.env_manager=env_manager\n",
        "\n",
        "        self.episodes_num=episodes_num\n",
        "        self.steps_per_episode=steps_per_episode\n",
        "\n",
        "        self.logs=None\n",
        "\n",
        "        self.output=None\n",
        "    \n",
        "    def reset_logs(self):\n",
        "        self.logs=dict({'episode':0,'step':0,'durations':[],'rewards':[],'exploration_rate':self.agent.explore_exploit_strat.get_exploration_rate(0),'last_policy':None,'last_action':None,'last_reward':None})\n",
        "\n",
        "    def update_logs(self,experience,counter,ep,step):\n",
        "        self.logs['episode']=ep\n",
        "        self.logs['step']=step\n",
        "\n",
        "        if(len(self.logs['durations'])<ep) : self.logs['durations'].append(step)\n",
        "        else :  self.logs['durations'][ep-1]=step\n",
        "        if(len(self.logs['rewards'])<ep) : self.logs['rewards'].append(experience[3])\n",
        "        else :  self.logs['rewards'][ep-1]+=experience[3]\n",
        "\n",
        "        self.logs['exploration_rate']=self.agent.explore_exploit_strat.get_exploration_rate(counter)\n",
        "        self.logs['last_policy']=self.agent.policy_net.policy\n",
        "        self.logs['last_action']=experience[1]\n",
        "        self.logs['last_reward']=experience[3]\n",
        "\n",
        "    def display_stats(self,mode):\n",
        "          \n",
        "        if(mode==\"train\"):\n",
        "\n",
        "            if(self.logs['step']>1):\n",
        "                sys.stdout.write(\"\\b\"*len(self.output))\n",
        "\n",
        "            self.output=\"\\n\\n\"\n",
        "            self.output+=\"\\nEpisode {}/{}\".format(self.logs['episode'],self.episodes_num)\n",
        "            self.output+=\"\\nstep {}/{}\".format(self.logs['step'],self.steps_per_episode)\n",
        "            self.output+=\"\\nexploration_rate:{}\".format(self.logs['exploration_rate'])\n",
        "            self.output+=\"\\npolicy:{}\".format(self.logs['last_policy'])\n",
        "            self.output+=\"\\naction:{}\".format(self.logs['last_action'])\n",
        "            self.output+=\"\\nreward:{}\".format(self.logs['last_reward'])\n",
        "\n",
        "            self.output+=\"\\ntotal episode reward:{}\".format(self.logs['rewards'][-1])\n",
        "            if(len(self.logs['durations'])>100) : self.output+=\"\\nlast 100 episodes durations moving avg:{}\".format(np.mean(np.array(self.logs['durations'][-100:])))\n",
        "            if(len(self.logs['rewards'])>100) : self.output+=\"\\nlast 100 episodes rewards moving avg:{}\".format(np.mean(np.array(self.logs['rewards'][-100:])))\n",
        "\n",
        "            sys.stdout.write(self.output)\n",
        "\n",
        "        elif(mode==\"test\"):\n",
        "\n",
        "            self.output=\"\\n\\n\"\n",
        "            self.output+=\"\\nEpisode {}/{}\".format(self.logs['episode'],self.episodes_num)\n",
        "            self.output+=\"\\nstep {}/{}\".format(self.logs['step'],self.steps_per_episode)\n",
        "            self.output+=\"\\npolicy:{}\".format(self.logs['last_policy'])\n",
        "            self.output+=\"\\naction:{}\".format(self.logs['last_action'])\n",
        "            self.output+=\"\\nreward:{}\".format(self.logs['last_reward'])\n",
        "\n",
        "            self.output+=\"\\ntotal episode reward:{}\".format(self.logs['rewards'][-1])\n",
        "            if(len(self.logs['durations'])>100) : self.output+=\"\\nlast 100 episodes durations moving avg:{}\".format(np.mean(np.array(self.logs['durations'][-100:])))\n",
        "            if(len(self.logs['rewards'])>100) : self.output+=\"\\nlast 100 episodes rewards moving avg:{}\".format(np.mean(np.array(self.logs['rewards'][-100:])))\n",
        "\n",
        "            sys.stdout.write(self.output)\n",
        "\n",
        "    \n",
        "    def start_train(self):\n",
        "        self.reset_logs()\n",
        "\n",
        "        self.env_manager.load()\n",
        "        counter=0\n",
        "        for ep in range(1,self.episodes_num+1):\n",
        "            self.env_manager.reset()\n",
        "\n",
        "            step=0\n",
        "            while(1):\n",
        "                step+=1;counter+=1\n",
        "\n",
        "                experience=agent.take_action(counter) \n",
        "\n",
        "                self.update_logs(experience,counter,ep,step)\n",
        "                self.display_stats(mode=\"train\")\n",
        "\n",
        "                agent.learn(experience,counter)\n",
        "\n",
        "                if(step==self.steps_per_episode or experience[4]==1):\n",
        "                    break\n",
        "\n",
        "        env_manager.close()\n",
        "        self.agent.policy_net.save_model()\n",
        "\n",
        "    def start_test(self,episodes_num=10):\n",
        "        self.episodes_num=episodes_num\n",
        "        self.reset_logs()\n",
        "\n",
        "        self.env_manager.load()\n",
        "        counter=0\n",
        "        for ep in range(1,episodes_num+1):\n",
        "            self.env_manager.reset()\n",
        "\n",
        "            step=0\n",
        "            while(1):\n",
        "                step+=1;counter+=1\n",
        "\n",
        "                experience=agent.take_action(10**10)\n",
        "\n",
        "                self.update_logs(experience,counter,ep,step)\n",
        "\n",
        "                self.env_manager.render()\n",
        "                self.display_stats(mode=\"test\")\n",
        "                time.sleep(0.1)\n",
        "\n",
        "                if(step==self.steps_per_episode or experience[4]==1):\n",
        "                    break\n",
        "        env_manager.close()\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nnkO-ojWU65o"
      },
      "source": [
        "env_manager=EnvManager(env_name='Pong-v0')\n",
        "\n",
        "policy_net=PolicyNet( load_name=\"\",save_name=\"\" )\n",
        "policy_net.build_model(input_shape=(210,160,3),output_shape=2,loss=\"mse\",learning_rate=0.001)\n",
        "target_net=TargetNet(policy_net=policy_net,target_update=100)\n",
        "target_net.build_model()\n",
        "\n",
        "replay_mem=ReplayMemory(capacity=100000,batch_size=64)\n",
        "explore_exploit_strat=ExploreExploitStrat( strategy=EpsilonGreedyStrategy(start=1.0,end=0.1,decay_rate=0.0001) )\n",
        "return_strat=ReturnStrat( strategy=DiscountedReturn(Y=0.99) )\n",
        "\n",
        "agent=Agent(env_manager,replay_mem,explore_exploit_strat,return_strat,policy_net,target_net)\n",
        "\n",
        "rl=RL(agent,env_manager,episodes_num=200,steps_per_episode=500)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u5Ka9FzIWXiS",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 946
        },
        "outputId": "1e2477c0-0f59-4ab5-b1c7-288d6f0a79fc"
      },
      "source": [
        "rl.start_train()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\n",
            "Episode 1/200\n",
            "step 1/500\n",
            "exploration_rate:0.99991000449985\n",
            "policy:None\n",
            "action:2\n",
            "reward:0.0\n",
            "total episode reward:0.0\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
            "\n",
            "\n",
            "Episode 1/200\n",
            "step 2/500\n",
            "exploration_rate:0.9998200179988\n",
            "policy:None\n",
            "action:2\n",
            "reward:0.0\n",
            "total episode reward:0.0\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
            "\n",
            "\n",
            "Episode 1/200\n",
            "step 3/500\n",
            "exploration_rate:0.9997300404959503\n",
            "policy:None\n",
            "action:0\n",
            "reward:0.0\n",
            "total episode reward:0.0\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
            "\n",
            "\n",
            "Episode 1/200\n",
            "step 4/500\n",
            "exploration_rate:0.999640071990401\n",
            "policy:None\n",
            "action:0\n",
            "reward:0.0\n",
            "total episode reward:0.0\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
            "\n",
            "\n",
            "Episode 1/200\n",
            "step 5/500\n",
            "exploration_rate:0.9995501124812524\n",
            "policy:None\n",
            "action:0\n",
            "reward:0.0\n",
            "total episode reward:0.0\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
            "\n",
            "\n",
            "Episode 1/200\n",
            "step 6/500\n",
            "exploration_rate:0.9994601619676049\n",
            "policy:None\n",
            "action:1\n",
            "reward:0.0\n",
            "total episode reward:0.0\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
            "\n",
            "\n",
            "Episode 1/200\n",
            "step 7/500\n",
            "exploration_rate:0.9993702204485589\n",
            "policy:None\n",
            "action:5\n",
            "reward:0.0\n",
            "total episode reward:0.0\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
            "\n",
            "\n",
            "Episode 1/200\n",
            "step 8/500\n",
            "exploration_rate:0.9992802879232153\n",
            "policy:None\n",
            "action:4\n",
            "reward:0.0\n",
            "total episode reward:0.0\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
            "\n",
            "\n",
            "Episode 1/200\n",
            "step 9/500\n",
            "exploration_rate:0.9991903643906747\n",
            "policy:None\n",
            "action:3\n",
            "reward:0.0\n",
            "total episode reward:0.0\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
            "\n",
            "\n",
            "Episode 1/200\n",
            "step 10/500\n",
            "exploration_rate:0.9991004498500375\n",
            "policy:None\n",
            "action:1\n",
            "reward:0.0\n",
            "total episode reward:0.0\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
            "\n",
            "\n",
            "Episode 1/200\n",
            "step 11/500\n",
            "exploration_rate:0.9990105443004048\n",
            "policy:None\n",
            "action:3\n",
            "reward:0.0\n",
            "total episode reward:0.0\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
            "\n",
            "\n",
            "Episode 1/200\n",
            "step 12/500\n",
            "exploration_rate:0.9989206477408777\n",
            "policy:None\n",
            "action:0\n",
            "reward:0.0\n",
            "total episode reward:0.0\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
            "\n",
            "\n",
            "Episode 1/200\n",
            "step 13/500\n",
            "exploration_rate:0.998830760170557\n",
            "policy:None\n",
            "action:4\n",
            "reward:0.0\n",
            "total episode reward:0.0\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
            "\n",
            "\n",
            "Episode 1/200\n",
            "step 14/500\n",
            "exploration_rate:0.998740881588544\n",
            "policy:None\n",
            "action:5\n",
            "reward:0.0\n",
            "total episode reward:0.0\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
            "\n",
            "\n",
            "Episode 1/200\n",
            "step 15/500\n",
            "exploration_rate:0.9986510119939398\n",
            "policy:None\n",
            "action:4\n",
            "reward:0.0\n",
            "total episode reward:0.0\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
            "\n",
            "\n",
            "Episode 1/200\n",
            "step 16/500\n",
            "exploration_rate:0.9985611513858457\n",
            "policy:None\n",
            "action:3\n",
            "reward:0.0\n",
            "total episode reward:0.0\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
            "\n",
            "\n",
            "Episode 1/200\n",
            "step 17/500\n",
            "exploration_rate:0.9984712997633631\n",
            "policy:None\n",
            "action:0\n",
            "reward:0.0\n",
            "total episode reward:0.0\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
            "\n",
            "\n",
            "Episode 1/200\n",
            "step 18/500\n",
            "exploration_rate:0.9983814571255936\n",
            "policy:None\n",
            "action:0\n",
            "reward:0.0\n",
            "total episode reward:0.0\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
            "\n",
            "\n",
            "Episode 1/200\n",
            "step 19/500\n",
            "exploration_rate:0.9982916234716385\n",
            "policy:None\n",
            "action:5\n",
            "reward:0.0\n",
            "total episode reward:0.0\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
            "\n",
            "\n",
            "Episode 1/200\n",
            "step 20/500\n",
            "exploration_rate:0.9982017988005998\n",
            "policy:None\n",
            "action:2\n",
            "reward:0.0\n",
            "total episode reward:0.0\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
            "\n",
            "\n",
            "Episode 1/200\n",
            "step 21/500\n",
            "exploration_rate:0.998111983111579\n",
            "policy:None\n",
            "action:3\n",
            "reward:0.0\n",
            "total episode reward:0.0\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
            "\n",
            "\n",
            "Episode 1/200\n",
            "step 22/500\n",
            "exploration_rate:0.9980221764036781\n",
            "policy:None\n",
            "action:5\n",
            "reward:0.0\n",
            "total episode reward:0.0\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
            "\n",
            "\n",
            "Episode 1/200\n",
            "step 23/500\n",
            "exploration_rate:0.9979323786759989\n",
            "policy:None\n",
            "action:3\n",
            "reward:0.0\n",
            "total episode reward:0.0\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
            "\n",
            "\n",
            "Episode 1/200\n",
            "step 24/500\n",
            "exploration_rate:0.9978425899276436\n",
            "policy:None\n",
            "action:0\n",
            "reward:0.0\n",
            "total episode reward:0.0\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
            "\n",
            "\n",
            "Episode 1/200\n",
            "step 25/500\n",
            "exploration_rate:0.9977528101577141\n",
            "policy:None\n",
            "action:5\n",
            "reward:0.0\n",
            "total episode reward:0.0\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
            "\n",
            "\n",
            "Episode 1/200\n",
            "step 26/500\n",
            "exploration_rate:0.9976630393653128\n",
            "policy:None\n",
            "action:3\n",
            "reward:0.0\n",
            "total episode reward:0.0\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
            "\n",
            "\n",
            "Episode 1/200\n",
            "step 27/500\n",
            "exploration_rate:0.9975732775495418\n",
            "policy:None\n",
            "action:5\n",
            "reward:0.0\n",
            "total episode reward:0.0\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
            "\n",
            "\n",
            "Episode 1/200\n",
            "step 28/500\n",
            "exploration_rate:0.9974835247095036\n",
            "policy:None\n",
            "action:2\n",
            "reward:0.0\n",
            "total episode reward:0.0\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
            "\n",
            "\n",
            "Episode 1/200\n",
            "step 29/500\n",
            "exploration_rate:0.9973937808443007\n",
            "policy:None\n",
            "action:5\n",
            "reward:0.0\n",
            "total episode reward:0.0\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
            "\n",
            "\n",
            "Episode 1/200\n",
            "step 30/500\n",
            "exploration_rate:0.9973040459530357\n",
            "policy:None\n",
            "action:1\n",
            "reward:0.0\n",
            "total episode reward:0.0\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
            "\n",
            "\n",
            "Episode 1/200\n",
            "step 31/500\n",
            "exploration_rate:0.997214320034811\n",
            "policy:None\n",
            "action:0\n",
            "reward:0.0\n",
            "total episode reward:0.0\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
            "\n",
            "\n",
            "Episode 1/200\n",
            "step 32/500\n",
            "exploration_rate:0.9971246030887296\n",
            "policy:None\n",
            "action:5\n",
            "reward:0.0\n",
            "total episode reward:0.0\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
            "\n",
            "\n",
            "Episode 1/200\n",
            "step 33/500\n",
            "exploration_rate:0.9970348951138942\n",
            "policy:None\n",
            "action:5\n",
            "reward:0.0\n",
            "total episode reward:0.0\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
            "\n",
            "\n",
            "Episode 1/200\n",
            "step 34/500\n",
            "exploration_rate:0.9969451961094079\n",
            "policy:None\n",
            "action:4\n",
            "reward:0.0\n",
            "total episode reward:0.0\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
            "\n",
            "\n",
            "Episode 1/200\n",
            "step 35/500\n",
            "exploration_rate:0.9968555060743733\n",
            "policy:None\n",
            "action:0\n",
            "reward:0.0\n",
            "total episode reward:0.0\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
            "\n",
            "\n",
            "Episode 1/200\n",
            "step 36/500\n",
            "exploration_rate:0.996765825007894\n",
            "policy:None\n",
            "action:2\n",
            "reward:0.0\n",
            "total episode reward:0.0\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
            "\n",
            "\n",
            "Episode 1/200\n",
            "step 37/500\n",
            "exploration_rate:0.9966761529090729\n",
            "policy:None\n",
            "action:2\n",
            "reward:0.0\n",
            "total episode reward:0.0\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
            "\n",
            "\n",
            "Episode 1/200\n",
            "step 38/500\n",
            "exploration_rate:0.9965864897770134\n",
            "policy:None\n",
            "action:0\n",
            "reward:0.0\n",
            "total episode reward:0.0\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
            "\n",
            "\n",
            "Episode 1/200\n",
            "step 39/500\n",
            "exploration_rate:0.9964968356108187\n",
            "policy:None\n",
            "action:2\n",
            "reward:0.0\n",
            "total episode reward:0.0\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
            "\n",
            "\n",
            "Episode 1/200\n",
            "step 40/500\n",
            "exploration_rate:0.9964071904095924\n",
            "policy:None\n",
            "action:3\n",
            "reward:0.0\n",
            "total episode reward:0.0\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
            "\n",
            "\n",
            "Episode 1/200\n",
            "step 41/500\n",
            "exploration_rate:0.9963175541724378\n",
            "policy:None\n",
            "action:5\n",
            "reward:0.0\n",
            "total episode reward:0.0\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
            "\n",
            "\n",
            "Episode 1/200\n",
            "step 42/500\n",
            "exploration_rate:0.9962279268984591\n",
            "policy:None\n",
            "action:2\n",
            "reward:0.0\n",
            "total episode reward:0.0\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
            "\n",
            "\n",
            "Episode 1/200\n",
            "step 43/500\n",
            "exploration_rate:0.9961383085867596\n",
            "policy:None\n",
            "action:4\n",
            "reward:0.0\n",
            "total episode reward:0.0\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
            "\n",
            "\n",
            "Episode 1/200\n",
            "step 44/500\n",
            "exploration_rate:0.996048699236443\n",
            "policy:None\n",
            "action:4\n",
            "reward:0.0\n",
            "total episode reward:0.0\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
            "\n",
            "\n",
            "Episode 1/200\n",
            "step 45/500\n",
            "exploration_rate:0.9959590988466135\n",
            "policy:None\n",
            "action:3\n",
            "reward:0.0\n",
            "total episode reward:0.0\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
            "\n",
            "\n",
            "Episode 1/200\n",
            "step 46/500\n",
            "exploration_rate:0.995869507416375\n",
            "policy:None\n",
            "action:1\n",
            "reward:0.0\n",
            "total episode reward:0.0\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
            "\n",
            "\n",
            "Episode 1/200\n",
            "step 47/500\n",
            "exploration_rate:0.9957799249448317\n",
            "policy:None\n",
            "action:1\n",
            "reward:0.0\n",
            "total episode reward:0.0\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
            "\n",
            "\n",
            "Episode 1/200\n",
            "step 48/500\n",
            "exploration_rate:0.9956903514310874\n",
            "policy:None\n",
            "action:4\n",
            "reward:0.0\n",
            "total episode reward:0.0\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
            "\n",
            "\n",
            "Episode 1/200\n",
            "step 49/500\n",
            "exploration_rate:0.9956007868742468\n",
            "policy:None\n",
            "action:2\n",
            "reward:0.0\n",
            "total episode reward:0.0\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
            "\n",
            "\n",
            "Episode 1/200\n",
            "step 50/500\n",
            "exploration_rate:0.9955112312734141\n",
            "policy:None\n",
            "action:1\n",
            "reward:0.0\n",
            "total episode reward:0.0\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
            "\n",
            "\n",
            "Episode 1/200\n",
            "step 51/500\n",
            "exploration_rate:0.9954216846276936\n",
            "policy:None\n",
            "action:4\n",
            "reward:0.0\n",
            "total episode reward:0.0\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
            "\n",
            "\n",
            "Episode 1/200\n",
            "step 52/500\n",
            "exploration_rate:0.9953321469361901\n",
            "policy:None\n",
            "action:5\n",
            "reward:0.0\n",
            "total episode reward:0.0\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
            "\n",
            "\n",
            "Episode 1/200\n",
            "step 53/500\n",
            "exploration_rate:0.995242618198008\n",
            "policy:None\n",
            "action:5\n",
            "reward:0.0\n",
            "total episode reward:0.0\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
            "\n",
            "\n",
            "Episode 1/200\n",
            "step 54/500\n",
            "exploration_rate:0.9951530984122521\n",
            "policy:None\n",
            "action:5\n",
            "reward:0.0\n",
            "total episode reward:0.0\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
            "\n",
            "\n",
            "Episode 1/200\n",
            "step 55/500\n",
            "exploration_rate:0.9950635875780272\n",
            "policy:None\n",
            "action:4\n",
            "reward:0.0\n",
            "total episode reward:0.0\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
            "\n",
            "\n",
            "Episode 1/200\n",
            "step 56/500\n",
            "exploration_rate:0.9949740856944381\n",
            "policy:None\n",
            "action:2\n",
            "reward:0.0\n",
            "total episode reward:0.0\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
            "\n",
            "\n",
            "Episode 1/200\n",
            "step 57/500\n",
            "exploration_rate:0.9948845927605898\n",
            "policy:None\n",
            "action:0\n",
            "reward:0.0\n",
            "total episode reward:0.0\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
            "\n",
            "\n",
            "Episode 1/200\n",
            "step 58/500\n",
            "exploration_rate:0.9947951087755877\n",
            "policy:None\n",
            "action:3\n",
            "reward:0.0\n",
            "total episode reward:0.0\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
            "\n",
            "\n",
            "Episode 1/200\n",
            "step 59/500\n",
            "exploration_rate:0.9947056337385365\n",
            "policy:None\n",
            "action:2\n",
            "reward:0.0\n",
            "total episode reward:0.0\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
            "\n",
            "\n",
            "Episode 1/200\n",
            "step 60/500\n",
            "exploration_rate:0.9946161676485418\n",
            "policy:None\n",
            "action:2\n",
            "reward:0.0\n",
            "total episode reward:0.0\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
            "\n",
            "\n",
            "Episode 1/200\n",
            "step 61/500\n",
            "exploration_rate:0.9945267105047085\n",
            "policy:None\n",
            "action:1\n",
            "reward:0.0\n",
            "total episode reward:0.0\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
            "\n",
            "\n",
            "Episode 1/200\n",
            "step 62/500\n",
            "exploration_rate:0.9944372623061426\n",
            "policy:None\n",
            "action:4\n",
            "reward:0.0\n",
            "total episode reward:0.0\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
            "\n",
            "\n",
            "Episode 1/200\n",
            "step 63/500\n",
            "exploration_rate:0.9943478230519492\n",
            "policy:None\n",
            "action:1\n",
            "reward:0.0\n",
            "total episode reward:0.0\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
            "\n",
            "\n",
            "Episode 1/200\n",
            "step 64/500\n",
            "exploration_rate:0.9942583927412341\n",
            "policy:None\n",
            "action:5\n",
            "reward:0.0\n",
            "total episode reward:0.0"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAKAAAADSCAIAAABCR1ywAAACZ0lEQVR4nO3dMUoDQRiAUVe8gHex0V4QvI0HES9jZa+NR5lD2G4TsoK7Qz7fa7MkAx8/A5MwWa5OeHu6PfUSF2Q5MuTjw/3ZZz4+vw5YyTG+X57PPnP3+r7rGq53fXemEzhO4LibWR+83mu37M2Xbr3Xbtmb/4oJjhM4TuA4geMEjhM4TuA4geMEjhM4TuC4aWfR/+H8ee3I8+c1ExwncJzAcYf+JovjmeA4geOWMcbsNbAjExwncJzAcQLHCRwncJzAcQLHCRzny4Y4ExwncJzAcQLHCRwncJzAcQLHCRwncJzAcQLHCRwncJzAcQLHCRwncJzAcQLHCRwncJzAcQLHCRwncJzAcQLHCRwncJzAcdMuBGe79WXiv/1DaRMcJ3CcwHECxwkcJ3CcwHECxwkcJ3CcwHECxwkcJ3CcwHHui44zwXECxwkcJ3CcwHECxwkcJ3CcwHECxwkcJ3CcwHECxwkcJ3CcwHECxwkcJ3CcwHECxwkcJ3CcwHECxwkcJ3CcwHECxwkcJ3CcwHECxwkcJ3CcwHECxwkcJ3CcwHECxwkcJ3CcwHECxwkcJ3CcwHECxwkcJ3CcwHECxwkcJ3CcwHECxwkcJ3CcwHECxwkcJ3CcwHECxwkcJ3CcwHECxwkcJ3CcwHECxwkcJ3CcwHECxwkcJ3CcwHECxwkcJ3CcwHECxwkcJ3CcwHECxwkcJ3CcwHECxwkcJ3CcwHECxwkcJ3CcwHECxwkcJ3CcwHECxwkcJ3CcwHECxwkct4wxZq+BHZngOIHjBI4TOE7gOIHjBI4TOE7gOIHjBI4TOE7gOIHjBI77AcMlHPrgebhNAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<PIL.Image.Image image mode=RGB size=160x210 at 0x7FAF7E287B00>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAKAAAADSCAIAAABCR1ywAAACZ0lEQVR4nO3dMUoDQRiAUVe8gHex0V4QvI0HES9jZa+NR5lD2G4TsoK7Qz7fa7MkAx8/A5MwWa5OeHu6PfUSF2Q5MuTjw/3ZZz4+vw5YyTG+X57PPnP3+r7rGq53fXemEzhO4LibWR+83mu37M2Xbr3Xbtmb/4oJjhM4TuA4geMEjhM4TuA4geMEjhM4TuC4aWfR/+H8ee3I8+c1ExwncJzAcYf+JovjmeA4geOWMcbsNbAjExwncJzAcQLHCRwncJzAcQLHCRzny4Y4ExwncJzAcQLHCRwncJzAcQLHCRwncJzAcQLHCRwncJzAcQLHCRwncJzAcQLHCRwncJzAcQLHCRwncJzAcdMuI2W79UWmv/0zSxMcJ3CcwHECxwkcJ3CcwHECxwkcJ3CcwHECxwkcJ3CcwHHuqowzwXECxwkcJ3CcwHECxwkcJ3CcwHECxwkcJ3CcwHECxwkcJ3CcwHECxwkcJ3CcwHECxwkcJ3CcwHECxwkcJ3CcwHECxwkcJ3CcwHECxwkcJ3CcwHECxwkcJ3CcwHECxwkcJ3CcwHECxwkcJ3CcwHECxwkcJ3CcwHECxwkcJ3CcwHECxwkcJ3CcwHECxwkcJ3CcwHECxwkcJ3CcwHECxwkcJ3CcwHECxwkcJ3CcwHECxwkcJ3CcwHECxwkcJ3CcwHECxwkcJ3CcwHECxwkcJ3CcwHECxwkcJ3CcwHECxwkcJ3CcwHECxwkcJ3CcwHECxwkcJ3CcwHECxwkcJ3CcwHECxwkct4wxZq+BHZngOIHjBI4TOE7gOIHjBI4TOE7gOIHjBI4TOE7gOIHjBI77AS+PHPpQEVwSAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<PIL.Image.Image image mode=RGB size=160x210 at 0x7FAF7E287B00>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "error",
          "ename": "IndexError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-10-49845acda539>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mrl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart_train\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-6-02886671257c>\u001b[0m in \u001b[0;36mstart_train\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    351\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdisplay_stats\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"train\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    352\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 353\u001b[0;31m                 \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlearn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexperience\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcounter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    354\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    355\u001b[0m                 \u001b[0;32mif\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msteps_per_episode\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mexperience\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-6-02886671257c>\u001b[0m in \u001b[0;36mlearn\u001b[0;34m(self, experience, current_step)\u001b[0m\n\u001b[1;32m    259\u001b[0m             \u001b[0mcurr_Qvals\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpolicy_net\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_Qvals\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobservations\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    260\u001b[0m             \u001b[0mnext_Qvals\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtarget_net\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_target_Qvals\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnext_observations\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcurrent_step\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 261\u001b[0;31m             \u001b[0mreturns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreturn_strat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_return\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrewards\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtake_along_axis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnext_Qvals\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mactions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mravel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m,\u001b[0m\u001b[0mdones\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcurrent_step\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    262\u001b[0m             \u001b[0mtarget_Qvals\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcurr_Qvals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    263\u001b[0m             \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mput_along_axis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget_Qvals\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0mactions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0mreturns\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<__array_function__ internals>\u001b[0m in \u001b[0;36mtake_along_axis\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/numpy/lib/shape_base.py\u001b[0m in \u001b[0;36mtake_along_axis\u001b[0;34m(arr, indices, axis)\u001b[0m\n\u001b[1;32m    170\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    171\u001b[0m     \u001b[0;31m# use the fancy index\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 172\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0marr\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0m_make_along_axis_idx\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marr_shape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindices\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    173\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    174\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mIndexError\u001b[0m: index 5 is out of bounds for axis 1 with size 2"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lbpm6KbFWdhq"
      },
      "source": [
        "rl.start_test(episodes_num=10)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}